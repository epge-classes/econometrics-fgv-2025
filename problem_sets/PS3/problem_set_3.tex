\documentclass[11pt]{article}
\usepackage[letterpaper,margin=2cm]{geometry}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\linespread{1.3}
\parskip=12pt
\parindent=0pt
\usepackage{enumerate}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage[section]{placeins}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{mathpazo}

\usepackage[dvipsnames]{xcolor}
\definecolor{slateblue}{RGB}{45, 62, 80}
\definecolor{FGVBlue}{RGB}{0, 114, 188}
\usepackage{hyperref}
\hypersetup{
	colorlinks=true,
	linkcolor=FGVBlue,
	urlcolor=FGVBlue,
	citecolor=FGVBlue,
}
% Defining the question styles
\theoremstyle{definition}
\newtheorem{prob}{Problem}

% Custom commands
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Prob}{\mathbb{P}}

% Command to start a new section with the word "Problem" and enumerate automatically
\newcounter{problem}
\renewcommand{\theproblem}{\arabic{problem}}

\newcommand{\problem}[1]{
	\stepcounter{problem}
	\section*{Problem \theproblem{} -- (points: #1)}
}

\begin{document}
	\begin{center}
		{\LARGE{\textbf{Problem Set III}}}\\
		\vspace{0.2cm}
		Econometrics I - \textcolor{FGVBlue}{FGV EPGE}\\
		Instructor: Raul Guarini Riva \\
		TA: Taric Latif Padovani
	\end{center}

\problem{2}
Consider an AR($p$) process:
\begin{equation}
	y_t - \mu = \phi_1 (y_{t-1} - \mu) + \phi_2 (y_{t-2} - \mu) + \cdots + \phi_p (y_{t-p} - \mu) + \varepsilon_t,
	\label{eq:ar_p}
\end{equation}
where $\varepsilon_t \sim \text{i.i.d. } (0, \sigma^2)$. This questions will explore the dynamics of conditional moments of $y_{t+h}$ given $\mathcal{I}_t$, where $\mathcal{I}_t = \{y_t, y_{t-1}, \ldots\}$ represents the information set available at time $t$.

\begin{enumerate}[a)]
	\item Consider $p\times 1$ vector $Y_t = (y_t - \mu, y_{t-1} - \mu, \ldots, y_{t-p+1} - \mu)'$. Show that there exists a $p \times p$ matrix $A$ and a $p \times 1$ vector $U_t$ such that:
	\[Y_t = A Y_{t-1} + U_t, \quad \forall t\]

	Additionally, show that $\Omega \equiv \mathbb{E}[U_tU_t']$ is a $p \times p$ matrix with all elements equal to zero, except for the first element of the main diagonal, which is equal to $\sigma^2$.

	\textit{Hint}: Matrix $A$ will only have 0's and 1's, except for the first row.

	\item Show that $\mathbb{E}[Y_{t+h}|\mathcal{I}_t] = A^h Y_t$.
	\item Find an expression for $\Var[Y_{t+h}|\mathcal{I}_t]$ that depends only on $A$, $\Omega$ and $h$;
	\item Show that the eigenvalues of $A$ are the roots of the polynomial \[\Phi(z) = (-1)^{p}\left(z^p - \phi_1 z^{p-1} - \phi_2 z^{p-2} - \cdots - \phi_p\right),\] i.e., this is its characteristic polynomial;
	
	\textit{Hint 1}: Recall that the determinant of a triangular matrix is equal to the product of its main diagonal elements.
	
	\textit{Hint 2}: Recall that if we multiply a column of a matrix by a constant and add the result to another column, the determinant does not change. Try applying operations on $(A - \lambda I)$ to make it triangular -- this is a good refresher in Linear Algebra, isn't it? 

	\item Even if you have not completed the previous item, argue that the eigenvalues of $A$ are all smaller than one in absolute value if the AR($p$) process is stationary;
	\item Find the limits of $\mathbb{E}[Y_{t+h}|\mathcal{I}_t]$ and $\Var[Y_{t+h}|\mathcal{I}_t]$ as $h \to \infty$ if the process is stationary. What is the intuition for this result?
\end{enumerate}

\problem{2}
In this question, we will explore one example of a stationary process that is not an ARMA process. Let $\psi_j = \frac{1}{j^2}$ for $j \neq 0$ and $\psi_0 = 1$. Consider the process $y_t$ defined in the following way:
\[
y_t = \sum_{j=0}^{\infty} \psi_j \varepsilon_{t-j}, \quad \varepsilon_t \sim \text{i.i.d. } (0, \sigma^2).
\]

\begin{enumerate}
	\item Compute the mean and variance of $y_t$;
	\item Compute the autocovariance function $\gamma(h) = Cov(y_t, y_{t-h})$ for $h = 1, 2, 3, \ldots$; Is this process covariance-stationary? Why?
	\item Show that there are positive constants $c_1$ and $c_2$ such that $c_1 \leq h^2 \cdot \gamma(h) \leq c_2$. Conclude that $\gamma(h) = O(1/h^2)$;
	\item Now, assume by contradiction that $y_t$ is an ARMA($p,q$) process for some finite $p$ and $q$. Let $\tilde{\gamma}(h)$ be the $h$-th autocovariance implied by the coefficients of this ARMA process. Show that 
	\[
	\lim_{h \to \infty}\frac{\gamma(h)}{|\tilde{\gamma}(h)|} = + \infty
	\]
	Conclue that $y_t$ can never be an ARMA($p,q$) process.
\end{enumerate}

\problem{6}
On the last problem set, you analyzed Brazilian inflation data. Now, you we will create a full pipeline for the estimation of an ARMA model for inflation and we will use it for forecasting. Please use the same dataset \texttt{ipca\_igpm.csv} as last time. We will concentrate on the IPCA time series for this problem set. We will model it as an ARMA($p,q$) process with unknown $p$ and $q$:

\begin{equation}
	y_t = c + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \cdots + \phi_p y_{t-p} + \varepsilon_t + \theta_1 \varepsilon_{t-1} + \theta_2 \varepsilon_{t-2} + \cdots + \theta_q \varepsilon_{t-q}
	\label{eq:arma_pq}
\end{equation}
where $\varepsilon_t \sim \text{i.i.d. } N(0, \sigma^2)$. We will assume that the process is stationary and invertible, so that the parameters are identified.

\begin{enumerate}[a)]
	\item Create a plot showing the time series evolution of the IPCA series;
	\item For given values of $p$ and $q$ (and the associated parameters), write a function that returns the conditional log-likelihood function of an ARMA($p,q$) model.
	\item Consider $p,q \in \{0,1,2\}$. For each of the 8 possible combinations of $p$ and $q$ (4 proper ARMA models, 2 AR models, and 2 MA models), estimate parameters by conditional maximum likelihood. Create a table in which you report, for each model, the estimated parameters, 95\% confidence intervals, and the AIC and BIC values;
	\item Based on the AIC and BIC values, which model would you choose? Why? Do they disagree in terms of the best model? If the model chosen by BIC is different than the one chosen by AIC, pick your favorite and justify your choice.
	\item For the model with the best possible fit, create two plots: (i) the fitted values against the actual series; (ii) the residuals of the model against time. Comment on these plots. 
	\item Create a plot showing the autocorrelation function of residuals. Comment on this plot;
	\item Now we will do some forecasting. For the model with the best fit, simulate 24 realizations of a random variable distributed as $N(0, \hat{\sigma}^2)$. Given the estimated parameters and these simulated shocks, iterate $y_t$ forward following \eqref{eq:arma_pq} to get a path of simulated future realizations of $y_t$. Plot the original series followed by your forecast.
	\item Repeat the item above 1000 times and keep track of the forecasts. For each point in time, compute the realizations in the $5\%$ and $95\%$ percentiles of the forecasts. Plot your forecast with these bounds around it.
	\item Intuitively, how similar is this process to the non-parametric boostrap we saw in the beginning of the class?
\end{enumerate}

\end{document}